\documentclass[703031]{iisreport}

\title{\textbf{Report}}
\author{Claudio Canella\\ Daniel Eberharter\\ Christoph Haas\\ Stefan Haselwanter}

\begin{document}
\maketitle


\section{Color detection}
\subsection{Approach}
For color detection we use HSV with the OpenCV method \emph{inRange}. We did not hard-code the colors, instead we retrieve the RGB color of a selected object by clicking on it and convert it to HSV (using algorithm discussed in computer graphics lecture). To be a little bit more independent of lightning and different shades of the same color we added an offset.

\subsection{Problems}
With color detection we ran into some problems, most of them being dependent on lightning and not our code.
	\begin{description}
		\item [Problem 1:] When we started with color detection we used the method discussed in the lecture with histograms and probability matrices, but this proved to be inefficient as the frame rate dropped to approximately 0.9 frames per second (this was without any additional computations as finding a bottom point). After switching to the HSV approach the frame rate increased to approximately 8 fps.
		\item [Problem 2:] We also encountered a problem with detecting colors when we hard-coded them as we always had to change the code depending on the light. That is why we implemented the teaching of the colors at runtime. \label{Color-Teaching}
	\end{description}


\section{Beacon detection}
\subsection{Approach}
To determine if we have detected a beacon first we are looking for the bounding rectangle of a color. In case of a detected color is above another one we check for the right color combinations of the beacons we saved in our code and if these two represents a valid beacon. As there is the possibility that the bounding rectangles do not touch each other based on the fact that due to lightning a perfect color detection is not possible, we have added a maximum distance that is allowed to be between the two color rectangles.

\subsection{Problems}

	\begin{description}
		\item [Problem 1:] The problem we had with beacon detection was that we detected many colors in the background that interfere with our beacons. That is why a bounding color rectangle must have at least an area of 1000, otherwise they are discarded.
		\item [Problem 2:] White also proved to be an unfortunate color for beacon detection as walls tend to be white and the criteria of a minimum area of 1000 also is true for them. That is why we decided to discard white as a valid beacon color. This brings the disadvantage that we can no longer use the beacons at position (0,75) and (150,75), but also reduces the probability of wrongly detected beacons and therefore increases our precision.
	\end{description}


\section{Self-localization \& orientation}
\label{sl}
\subsection{Approach}
For self-localization we use 2 beacons and simple trigonometry (\emph{law of cosine, law of sine} and \emph{Pythagorean theorem}) to calculate the robot position. We found this theorems easier to implement and understand as circle intersection, especially as we know the distance to the beacons and the distance between them.
Further after we calculate our distance from the left beacon (x-axis) and from the top (y-axis) we have to differentiate different cases depending on the detected beacons to get the correct position.

For the orientation we also use trigonometry. We assume the center of the camera's screen as a reference point for the robot's field of view. If we detect a beacon we can determine the angle (and thus the orientation of the robot) in which the robot is located using the robot's position, the position of the detected beacon, the distance between the robot and the beacon and a simple sine/cosine arithmetic.

\subsection{Problems}
At first we tried to implement self-localization using circle intersection, but somehow ran into problems as the function always returned either NaN or $\pm \infty$. So instead of wasting too much time trying to fix it we decided to switch to trigonometry which worked almost right away and is also a little bit faster.

\section{Motion control}
\subsection{Approach}


\subsection{Problems}


\section{Caging a ball}


\section{Usage of program}
The usage of the program depends on what the robot should do, but some steps are the same. Therefore we first discuss the steps that have to be done for all actions then the steps for each case.\\\\
\emph{Basic Steps:}
	\begin{enumerate}
		\item \textbf{Calc Homography:} If this takes quite some time, then the homography matrix could not be calculated, so please try again.
		\item \textbf{Calibrate Colors:} Name of color to be selected is displayed on the screen, aim with the crosshair at the correct color and touch the screen.
		\item \textbf{Calibrate Robot:} Put the green ball in front of the robot(10-15cm) so that it is centered on the screen. The robot will then perform turns and forward-backward movement to calculate a movement- and turn-factor for the robots servos.\\
	\end{enumerate}
\emph{Move to coordinates steps:}
	\begin{enumerate}
		\item \textbf{Move to...:} Specify coordinates in the format [x:y]
		\item \textbf{Toggle Tracking}\\
	\end{enumerate}
\emph{Cagging a ball steps:}
	\begin{enumerate}
		\item \textbf{Toggle catch object}
	\end{enumerate}
\end{document}